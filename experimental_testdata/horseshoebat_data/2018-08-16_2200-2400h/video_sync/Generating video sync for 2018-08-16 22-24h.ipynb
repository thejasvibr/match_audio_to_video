{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook details how I went about creating the video sync file for the 2 hours that have been audio and video annotated. This is a nice region of the data to work with because I can implement video -> audio and audio -> video matching. \n",
    "\n",
    "November 2019 \n",
    "\n",
    "    Thejasvi Beleyur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_folder = '/home/tbeleyur/Documents/packages_dev/match_audio_to_video/bin/'\n",
    "import datetime as dt\n",
    "import os\n",
    "import sys \n",
    "sys.path.append(module_folder)\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The RPi synchroniser positions:\n",
    "The RPi synchroniser sytem was moved at 22:13:15 (frame 30,000) ish in the video. Therefore, I'd rather only give the coordinates for LED tracking after 22:13:15. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera 2 LED and timestamp borders\n",
    "%run \"/home/tbeleyur/Documents/packages_dev/match_audio_to_video/bin/browse_through_video.py\" -v '/media/tbeleyur/THEJASVI_DATA_BACKUP_3/fieldwork_2018_002/horseshoe_bat/video/Horseshoe_bat_2018-08/2018-08-16/cam02/OrlovaChukaDome_02_20180816_21.50.31-23.00.00[R][@afd][1].avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run \"/home/tbeleyur/Documents/packages_dev/match_audio_to_video/bin/browse_through_video.py\" -v '/media/tbeleyur/THEJASVI_DATA_BACKUP_3/fieldwork_2018_002/horseshoe_bat/video/Horseshoe_bat_2018-08/2018-08-16/cam01/OrlovaChukaDome_01_20180816_21.50.31-23.00.00[R][@afc][0].avi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: The LED and Timestamp borders:\n",
    "#### Camera 2 : from frame 90,000.\n",
    "\n",
    "LED border is : (250.39286610091725, 898.4368744536677, 684.2887944886477, 167.9543277023157)\n",
    "\n",
    "Timestamp border is: (590.4865253498716, 55.153357631379635, 112.86677643519215, 992.9494364196597)\n",
    "\n",
    "#### Camera 1: from frame 90,002\n",
    "\n",
    "LED border is : (877.1086387785767, 1029.3501970284728, 52.26985521857125, 30.71657799107652)\n",
    "\n",
    "Timestamps border is: (587.0770547300873, 57.70633435385497, 114.79017806746924, 998.8518903900261)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reading video and extracting LED signal + timestamps\n",
    "Let us look at the annotations and which video files they refer to. The LED + timestamps for each video file that is mentioned at least once is extracted. \n",
    "\n",
    "This time I've actually not run the led and timestamp extraction through the Jupyter notebook because sometimes it gets interrupted. It is much more reliable calling a script from the command line - and thus I'm doing this. Right now I've run the data extraction for the first 200 frames on both videos - let's check out the LED signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam01_led = ['videosync_OrlovaChukaDome_01_20180816_21.50.31-23.00.00[R][@afc][0].avi_.csv',\n",
    "             'videosync_OrlovaChukaDome_01_20180816_23.00.00-00.00.00[R][@f6b][1].avi_.csv']\n",
    "\n",
    "cam02_led = ['videosync_OrlovaChukaDome_02_20180816_21.50.31-23.00.00[R][@afd][1].avi_.csv',\n",
    "             'videosync_OrlovaChukaDome_02_20180816_23.00.00-00.00.00[R][@f6a][0].avi_.csv']\n",
    "\n",
    "get_led = lambda X : pd.read_csv(X)['led_intensity']\n",
    "\n",
    "cam01_ledsignal = map(get_led, cam01_led)\n",
    "cam02_ledsignal = map(get_led, cam02_led)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fddf62a9490>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(cam01_ledsignal[0],label='cam1 video1')\n",
    "plt.plot(cam01_ledsignal[1],label='cam1 video2')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fddf5297490>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(cam02_ledsignal[0],label='cam2 video1')\n",
    "plt.plot(cam02_ledsignal[1],label='cam2 video2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "horseshoebat",
   "language": "python",
   "name": "horseshoebat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
